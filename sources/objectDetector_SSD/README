################################################################################
# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
#
# NVIDIA Corporation and its licensors retain all intellectual property
# and proprietary rights in and to this software, related documentation
# and any modifications thereto.  Any use, reproduction, disclosure or
# distribution of this software and related documentation without an express
# license agreement from NVIDIA Corporation is strictly prohibited.
#
################################################################################

This sample shows how to integrate a detector Model like SSD having layers
supported through IPlugin interface by TensorRT and with custom output layer
parsing for detected objects with DeepStreamSDK. The sample uses the
IPluginCreator interface for configuring the plugins.

--------------------------------------------------------------------------------
Sample contents:
- deepstream_app_config_ssd.txt - DeepStream reference app configuration file
  for using SSD as the primary detector
- config_infer_primary_ssd.txt - Configuration file for the GStreamer nvinfer
  plugin for the SSD model
- nvdsinfer_custom_impl_ssd/nvdsiplugin_ssd.cpp - IPluginV2+IPluginCreator
  implementation for "FlattenConcat" layer in the SSD model. This implementation
  has been referred to from the TensorRT sample sampleUffSSD.
- nvdsinfer_custom_impl_ssd/nvdsparsebbox_ssd.cpp - Output layer parsing
  function for detected objects for the SSD model.

--------------------------------------------------------------------------------
Pre-requisites:
- Copy the model's label file "ssd_coco_labels.txt" from the data/ssd directory
  in TensorRT samples to this directory.
- Steps to generate the UFF model from ssd_inception_v2_coco TensorFlow frozen
  graph. These steps have been referred from TensorRT sampleUffSSD README:
  1. Make sure TensorRT's uff-converter-tf package is installed.
  2. Install tensorflow-gpu package for python:
     For dGPU:
       $ pip install tensorflow-gpu
     For Jetson, refer to https://elinux.org/Jetson_Zoo#TensorFlow
  3. Download and untar the ssd_inception_v2_coco TensorFlow trained model from
     http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz
  4. Navigate to the extracted directory and convert the frozen graph to uff:
     $ cd ssd_inception_v2_coco_2017_11_17
     $ python /usr/lib/python2.7/dist-packages/uff/bin/convert_to_uff.py \
         frozen_inference_graph.pb -O NMS \
         -p /usr/src/tensorrt/samples/sampleUffSSD/config.py \
         -o sample_ssd_relu6.uff
  5. Copy sample_ssd_relu6.uff to this directory.

--------------------------------------------------------------------------------
Compile the custom library:
  $ make -C nvdsinfer_custom_impl_ssd

--------------------------------------------------------------------------------
Run the sample:
The "nvinfer" config file config_infer_primary_ssd.txt specifies the path to
the custom library and the custom output parsing function through the properties
"custom-lib-path" and "parse-bbox-func-name" respectively.

- With gst-launch-1.0
  For Jetson:
  $ gst-launch-1.0 filesrc location=../../samples/streams/sample_1080p_h264.mp4 ! \
        decodebin ! m.sink_0 nvstreammux name=m batch-size=1 width=1280 \
        height=720 ! nvinfer config-file-path= config_infer_primary_ssd.txt ! \
        nvvideoconvert ! nvdsosd ! nvegltransform ! nveglglessink
  For dGPU:
  $ gst-launch-1.0 filesrc location=../../samples/streams/sample_1080p_h264.mp4 ! \
        decodebin ! m.sink_0 nvstreammux name=m batch-size=1 width=1280 \
        height=720 ! nvinfer config-file-path= config_infer_primary_ssd.txt ! \
        nvvideoconvert ! nvdsosd ! nveglglessink

- With deepstream-app
  $ deepstream-app -c deepstream_app_config_ssd.txt

NOTE: To use INT8 mode, the INT8 calibration file for the SSD model needs to be
provided along with changing the network-mode to 1 in config_infer_primary_ssd.txt.
Refer to sampleUffSSD for running the sample in INT8 mode. The sample writes the
calibration cache to file "CalibrationTableSSD".
